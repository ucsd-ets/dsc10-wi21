{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import babypandas as bpd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example 1: Fighting Professors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Two professors are fighting about who is a better teacher. To settle the matter, they decide to give each of their classes the same exam. Whoever's class performs better will be considered the best teacher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = bpd.read_csv('data/scores.csv')\n",
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which professor (A or B) appears to have \"won\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores.groupby('prof').mean() # SOLUTION NO PROMPT\n",
    "\n",
    "#ANSWER\n",
    "won_prof = 'A' # SOLUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST ##\n",
    "won_prof == 'A'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Question 1\n",
    "The winning professor claims that they are significantly better than the other professor -- and it isn't just due to random chance. What technique can we use to evaluate their claim?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:\n",
    "\n",
    "Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Question 2\n",
    "\n",
    "What are the null and alternative hypotheses?\n",
    "\n",
    "- **Null**:\n",
    "- **Alternative**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:\n",
    "\n",
    "Null hypothesis: In the population, the distribution of scores of students under professor A is the same for students under professor B. The difference in the sample is due to chance.\n",
    "\n",
    "Alternative hypothesis: In the population, the distribution of scores of students under professor A have higher scores, on average, than the students under professor B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Question 3\n",
    "\n",
    "What test statistic can we use? Remember: it is usually better for *large* values of the test statistic to point towards the alternative hypothesis.\n",
    "\n",
    "**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**:\n",
    "\n",
    "Difference in Means between student scores under professor A and professor B. The higher the difference the more the statistic leans towards professor A i.e the alternative hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "What was the *observed value* of your test statistic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = scores.groupby('prof').mean().get('score').loc['A'] - scores.groupby('prof').mean().get('score').loc['B'] # SOLUTION\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "math.isclose(obs, 4.7411054460319235, rel_tol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Question 5\n",
    "\n",
    "Implement your chosen technique to test whether the null hypothesis should be rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_simulations = 1000\n",
    "simulated_stats = np.array([]) # SOLUTION\n",
    "# BEGIN SOLUTION NO PROMPT\n",
    "for _ in range(num_simulations):\n",
    "    shuffled_scores = np.random.permutation(scores.get('score'))\n",
    "    shuffled = scores.assign(shuffled_score = shuffled_scores)\n",
    "    group_means = shuffled.groupby('prof').mean().get('shuffled_score')\n",
    "    difference = group_means.loc['A'] - group_means.loc['B']\n",
    "    simulated_stats = np.append(simulated_stats, difference)\n",
    "# END SOLUTION\n",
    "simulated_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Question 6\n",
    "\n",
    "What is the probability that we see our observed value of the test statistic if the null hypothesis is true?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val = (simulated_stats >= obs).mean() # SOLUTION\n",
    "p_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_val < .05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Question 7\n",
    "\n",
    "The \"winning\" professor claims that the results show that they are the better teacher. Is this correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_true_or_false = True # SOLUTION\n",
    "claim_true_or_false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST ##\n",
    "claim_true_or_false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Example 2: Fun with Test Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Question 8\n",
    "\n",
    "You want to test whether a coin is fair. Your hypotheses are:\n",
    "\n",
    "- **Null**: the coin is fair\n",
    "- **Alternative**: the coin is not fair\n",
    "\n",
    "You'll flip the coin 100 times. What test statistic should you use to assess your claim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill out the following code to set up this experiment\n",
    "\n",
    "num_flips = 100 # SOLUTION\n",
    "\n",
    "# model the probability of our coin\n",
    "model = np.array([0.5, 0.5]) # SOLUTION\n",
    "\n",
    "# flip our coin ... times\n",
    "flip_outcomes = np.random.multinomial(num_flips, model) # SOLUTION\n",
    "\n",
    "# flip_outcomes = [num_heads, num_tails]\n",
    "num_heads = flip_outcomes[0]\n",
    "\n",
    "# What is our test statistic?\n",
    "def test_statistic(num_heads):\n",
    "    return np.abs(num_heads - 50) # SOLUTION\n",
    "\n",
    "# compute test statistic\n",
    "print(f\"Test statistic result : {test_statistic(num_heads)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Question 9\n",
    "\n",
    "In your experiment, you saw 61 heads. What is the observed value of your test statistic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_heads_experiment = 61\n",
    "\n",
    "observed_test_statistic = test_statistic(num_heads_experiment) # SOLUTION\n",
    "\n",
    "print(f\"Test statistic result : {observed_test_statistic}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Question 10\n",
    "\n",
    "You want to test whether an *n*-sided die is fair. Your hypotheses are:\n",
    "\n",
    "- **Null**: the die is fair\n",
    "- **Alternative**: the die is not fair\n",
    "\n",
    "You'll roll the die 100 times. What test statistic should you use to assess your claim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill out the following code to set up this experiment\n",
    "\n",
    "\n",
    "# specify number of sides\n",
    "N = 20\n",
    "num_rolls = 100 # SOLUTION\n",
    "\n",
    "# model the probability of our die\n",
    "model_die = np.array([1/N]*N) # SOLUTION\n",
    "\n",
    "# roll our die ... times\n",
    "roll_outcomes = np.random.multinomial(num_rolls, model_die) # SOLUTION\n",
    "\n",
    "# roll_outcomes = [count_num_side_1 ,..., ..., count_num_side_N]\n",
    "# roll_outcomes_prob = [perc_num_side_1 ,..., ..., perc_num_side_N]\n",
    "\n",
    "roll_outcomes_prob = roll_outcomes / num_rolls # SOLUTION\n",
    "\n",
    "\n",
    "# What is our test statistic?\n",
    "def test_statistic_die(roll_outcomes_prob, model_die):\n",
    "    return np.abs(roll_outcomes_prob - model_die).sum() / 2 # SOLUTION\n",
    "\n",
    "# total variation distance\n",
    "\n",
    "# compute test statistic\n",
    "print(f\"Test statistic result : {test_statistic_die(roll_outcomes_prob, model_die)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Question 11\n",
    "\n",
    "You rolled a 4-sided side 100 times and got \"one\" 20 times, \"two\" 30 times, \"three\" 40 times, and \"four\" 10 times. What is the observed value of your test statistic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify number of sides\n",
    "N = 4\n",
    "num_rolls = 100 # SOLUTION\n",
    "\n",
    "# Given roll outcomes\n",
    "roll_outcomes = np.array([20, 30, 40, 10]) \n",
    "roll_outcomes_prob = roll_outcomes / num_rolls # SOLUTION\n",
    "\n",
    "# model the probability of our die\n",
    "model_die = np.array([1/N]*N) # SOLUTION\n",
    "\n",
    "# compute the test statistic\n",
    "test_statistic = test_statistic_die(roll_outcomes_prob, model_die) # SOLUTION\n",
    "\n",
    "# display results\n",
    "print(f\"Test statistic result : {test_statistic_die(roll_outcomes_prob, model_die)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Question 12\n",
    "\n",
    "You rolled a 2-sided die 100 times and got \"one\" 61 times and \"two\" 39 times. What is the observed value of your test statistic?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify number of sides\n",
    "N = 2\n",
    "num_rolls = 100 # SOLUTION\n",
    "\n",
    "# Given roll outcomes\n",
    "roll_outcomes = np.array([61, 39]) \n",
    "roll_outcomes_prob = roll_outcomes / num_rolls # SOLUTION\n",
    "\n",
    "# model the probability of our die\n",
    "model_die = np.array([1/N]*N) # SOLUTION\n",
    "\n",
    "# compute the test statistic\n",
    "test_statistic = test_statistic_die(roll_outcomes_prob, model_die) # SOLUTION\n",
    "\n",
    "# display results\n",
    "print(f\"Test statistic result : {test_statistic_die(roll_outcomes_prob, model_die)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
